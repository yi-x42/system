"""
å½±ç‰‡æ¨™è¨»æœå‹™ - ç”Ÿæˆå¸¶æœ‰æª¢æ¸¬è³‡è¨Šçš„æ¨™è¨»å½±ç‰‡
"""

import cv2
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import json
from datetime import datetime
import colorsys
import math

try:
    from app.core.logger import main_logger as logger
except ImportError:
    import logging
    logger = logging.getLogger(__name__)

try:
    from ultralytics import YOLO
    from app.core.paths import resolve_model_path
except ImportError:
    logger.error("YOLO import failed")
    YOLO = None


class VideoAnnotationService:
    """å½±ç‰‡æ¨™è¨»æœå‹™ - åœ¨å½±ç‰‡ä¸Šæ¨™è¨»æª¢æ¸¬çµæœ"""
    
    def __init__(self, model_path: str = "yolo11n.pt"):
        self.colors = {}  # ç‰©ä»¶IDå°æ‡‰çš„é¡è‰²
        self.trail_history = {}  # ç§»å‹•è»Œè·¡æ­·å²
        self.max_trail_length = 30  # è»Œè·¡æœ€å¤§é•·åº¦
        
        # åˆå§‹åŒ–YOLOæ¨¡å‹
        try:
            if YOLO is None:
                raise Exception("YOLO not available")
            
            # æª¢æŸ¥æ¨¡å‹æª”æ¡ˆ
            # è§£ææ¨¡å‹è·¯å¾‘ (å…è¨±å‚³å…¥ç°¡å–®æª”å)
            model_path = resolve_model_path(model_path)
            model_file = Path(model_path)
            if not model_file.exists():
                # å˜—è©¦åœ¨ç•¶å‰ç›®éŒ„æŸ¥æ‰¾
                current_dir_model = Path.cwd() / model_path
                if current_dir_model.exists():
                    model_path = str(current_dir_model)
                else:
                    logger.warning(f"Model file not found: {model_path}, will download automatically")
            
            logger.info(f"Loading YOLO model: {model_path}")
            self.model = YOLO(model_path)
            logger.info(f"YOLO model loaded successfully: {model_path}")
            
        except Exception as e:
            logger.error(f"Failed to load YOLO model: {e}")
            raise Exception(f"YOLO model initialization failed: {e}")
        
        logger.info("VideoAnnotationService initialized successfully")
        
    def generate_annotated_video(self, input_video_path: str, output_video_path: str = None) -> Dict:
        """
        ç”Ÿæˆæ¨™è¨»å¾Œçš„å½±ç‰‡
        
        Args:
            input_video_path: è¼¸å…¥å½±ç‰‡è·¯å¾‘
            output_video_path: è¼¸å‡ºå½±ç‰‡è·¯å¾‘ï¼Œå¦‚æœç‚ºNoneå‰‡è‡ªå‹•ç”Ÿæˆ
            
        Returns:
            åŒ…å«è™•ç†çµæœçš„å­—å…¸
        """
        try:
            input_path = Path(input_video_path)
            if not input_path.exists():
                raise FileNotFoundError(f"è¼¸å…¥å½±ç‰‡ä¸å­˜åœ¨: {input_video_path}")
            
            # è‡ªå‹•ç”Ÿæˆè¼¸å‡ºè·¯å¾‘
            if output_video_path is None:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                output_dir = Path("annotated_videos")
                output_dir.mkdir(exist_ok=True)
                output_video_path = output_dir / f"annotated_{input_path.stem}_{timestamp}.mp4"
            else:
                output_video_path = Path(output_video_path)
                output_video_path.parent.mkdir(parents=True, exist_ok=True)
            
            logger.info(f"é–‹å§‹ç”Ÿæˆæ¨™è¨»å½±ç‰‡: {input_video_path} -> {output_video_path}")
            print(f"ğŸ¬ é–‹å§‹è™•ç†å½±ç‰‡: {input_path.name}")
            print(f"ğŸ“ è¼¸å‡ºä½ç½®: {output_video_path}")
            
            # é–‹å•Ÿè¼¸å…¥å½±ç‰‡
            cap = cv2.VideoCapture(str(input_video_path))
            if not cap.isOpened():
                raise Exception(f"ç„¡æ³•é–‹å•Ÿå½±ç‰‡: {input_video_path}")
            
            # ç²å–å½±ç‰‡è³‡è¨Š
            fps = int(cap.get(cv2.CAP_PROP_FPS))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            # è¨­å®šè¼¸å‡ºå½±ç‰‡
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(str(output_video_path), fourcc, fps, (width, height))
            
            if not out.isOpened():
                # å˜—è©¦ä¸åŒçš„ç·¨è§£ç¢¼å™¨
                print("âš ï¸  å˜—è©¦ä½¿ç”¨ XVID ç·¨è§£ç¢¼å™¨")
                fourcc = cv2.VideoWriter_fourcc(*'XVID')
                output_video_path = output_video_path.with_suffix('.avi')
                out = cv2.VideoWriter(str(output_video_path), fourcc, fps, (width, height))
                
                if not out.isOpened():
                    raise Exception(f"ç„¡æ³•å»ºç«‹è¼¸å‡ºå½±ç‰‡: {output_video_path}")
            
            frame_count = 0
            processed_frames = 0
            
            logger.info(f"å½±ç‰‡è³‡è¨Š: {width}x{height}, {fps}FPS, {total_frames}å¹€")
            logger.info(f"YOLOæ¨¡å‹ç‹€æ…‹: {self.model}")
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    logger.info("å½±ç‰‡è®€å–å®Œç•¢")
                    break
                
                try:
                    # é€²è¡Œç‰©ä»¶æª¢æ¸¬ (ä¸ä½¿ç”¨trackingé¿å…lapå•é¡Œ)
                    results = self.model(frame, verbose=False)
                    
                    # æ¨™è¨»å½±ç‰‡å¹€
                    annotated_frame = self._annotate_frame(frame, results, frame_count)
                    
                    # å¯«å…¥è¼¸å‡ºå½±ç‰‡
                    out.write(annotated_frame)
                    
                except Exception as frame_error:
                    logger.warning(f"è™•ç†å¹€ {frame_count} æ™‚ç™¼ç”ŸéŒ¯èª¤: {frame_error}")
                    # å¦‚æœè™•ç†å¤±æ•—ï¼Œç›´æ¥å¯«å…¥åŸå§‹å¹€
                    out.write(frame)
                
                frame_count += 1
                processed_frames += 1
                
                # æ¯è™•ç†50å¹€è¼¸å‡ºä¸€æ¬¡é€²åº¦
                if processed_frames % 50 == 0:
                    progress = (processed_frames / total_frames) * 100 if total_frames > 0 else 0
                    print(f"ğŸ“ˆ è™•ç†é€²åº¦: {processed_frames}/{total_frames} ({progress:.1f}%)")
                    logger.info(f"è™•ç†é€²åº¦: {processed_frames}/{total_frames} ({progress:.1f}%)")
            
            # é‡‹æ”¾è³‡æº
            cap.release()
            out.release()
            cv2.destroyAllWindows()  # æ¸…ç†æ‰€æœ‰OpenCVçª—å£
            
            # ç¢ºèªæª”æ¡ˆæ˜¯å¦æˆåŠŸç”Ÿæˆ
            output_path = Path(output_video_path)
            if not output_path.exists():
                raise Exception(f"è¼¸å‡ºå½±ç‰‡æª”æ¡ˆæœªç”Ÿæˆ: {output_video_path}")
            
            file_size = output_path.stat().st_size
            if file_size == 0:
                raise Exception(f"è¼¸å‡ºå½±ç‰‡æª”æ¡ˆç‚ºç©º: {output_video_path}")
            
            print(f"âœ… æ¨™è¨»å½±ç‰‡ç”ŸæˆæˆåŠŸ!")
            print(f"ğŸ“ æª”æ¡ˆä½ç½®: {output_video_path}")
            print(f"ğŸ“Š æª”æ¡ˆå¤§å°: {file_size / 1024 / 1024:.2f} MB")
            
            result = {
                "status": "success",
                "input_video": str(input_video_path),
                "output_video": str(output_video_path),
                "total_frames": total_frames,
                "processed_frames": processed_frames,
                "video_info": {
                    "width": width,
                    "height": height,
                    "fps": fps,
                    "duration": total_frames / fps if fps > 0 else 0
                }
            }
            
            logger.info(f"æ¨™è¨»å½±ç‰‡ç”Ÿæˆå®Œæˆ: {output_video_path}")
            return result
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ¨™è¨»å½±ç‰‡å¤±æ•—: {e}")
            raise
        finally:
            if 'cap' in locals():
                cap.release()
            if 'out' in locals():
                out.release()
    
    def _annotate_frame(self, frame: np.ndarray, results, frame_number: int) -> np.ndarray:
        """æ¨™è¨»å–®ä¸€å¹€"""
        annotated_frame = frame.copy()
        
        if not results or len(results) == 0:
            return self._add_frame_info(annotated_frame, frame_number, 0)
        
        detections = []
        
        # è§£ææª¢æ¸¬çµæœ
        for result in results:
            if result.boxes is not None:
                boxes = result.boxes
                for i in range(len(boxes)):
                    detection = {
                        'bbox': boxes.xyxy[i].cpu().numpy(),
                        'confidence': float(boxes.conf[i].cpu().numpy()),
                        'class_id': int(boxes.cls[i].cpu().numpy()),
                        'class_name': result.names[int(boxes.cls[i].cpu().numpy())],
                        'track_id': int(boxes.id[i].cpu().numpy()) if boxes.id is not None else None
                    }
                    detections.append(detection)
        
        # æ¨™è¨»æ¯å€‹æª¢æ¸¬ç‰©ä»¶
        for detection in detections:
            annotated_frame = self._draw_detection(annotated_frame, detection, frame_number)
        
        # æ·»åŠ æ•´é«”è³‡è¨Š
        annotated_frame = self._add_frame_info(annotated_frame, frame_number, len(detections))
        
        return annotated_frame
    
    def _draw_detection(self, frame: np.ndarray, detection: Dict, frame_number: int) -> np.ndarray:
        """ç¹ªè£½å–®å€‹æª¢æ¸¬ç‰©ä»¶çš„æ¨™è¨»"""
        bbox = detection['bbox']
        track_id = detection.get('track_id')
        class_name = detection['class_name']
        confidence = detection['confidence']
        
        x1, y1, x2, y2 = map(int, bbox)
        center_x = (x1 + x2) // 2
        center_y = (y1 + y2) // 2
        
        # ç²å–ç‰©ä»¶é¡è‰²
        color = self._get_object_color(track_id if track_id else detection['class_id'])
        
        # ç¹ªè£½é‚Šç•Œæ¡†
        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
        
        # æ›´æ–°ç§»å‹•è»Œè·¡
        if track_id is not None:
            self._update_trail(track_id, (center_x, center_y))
            # ç¹ªè£½ç§»å‹•è»Œè·¡
            self._draw_trail(frame, track_id, color)
            # è¨ˆç®—å’Œç¹ªè£½ç§»å‹•æ–¹å‘
            direction = self._calculate_direction(track_id)
            if direction:
                self._draw_direction_arrow(frame, (center_x, center_y), direction, color)
        
        # æº–å‚™æ¨™ç±¤æ–‡å­—
        if track_id is not None:
            label = f"ID:{track_id} {class_name}"
        else:
            label = f"{class_name}"
        
        label += f" {confidence:.2f}"
        
        # è¨ˆç®—æ–‡å­—å°ºå¯¸
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.6
        thickness = 2
        (text_w, text_h), baseline = cv2.getTextSize(label, font, font_scale, thickness)
        
        # ç¹ªè£½æ–‡å­—èƒŒæ™¯
        cv2.rectangle(frame, (x1, y1 - text_h - baseline - 10), 
                     (x1 + text_w, y1), color, -1)
        
        # ç¹ªè£½æ–‡å­—
        cv2.putText(frame, label, (x1, y1 - baseline - 5),
                   font, font_scale, (255, 255, 255), thickness)
        
        # ç¹ªè£½ä¸­å¿ƒé»
        cv2.circle(frame, (center_x, center_y), 4, color, -1)
        cv2.circle(frame, (center_x, center_y), 4, (255, 255, 255), 1)
        
        # æ·»åŠ é€Ÿåº¦è³‡è¨Šï¼ˆå¦‚æœæœ‰ç§»å‹•è»Œè·¡ï¼‰
        if track_id is not None and len(self.trail_history.get(track_id, [])) > 1:
            speed = self._calculate_speed(track_id)
            if speed is not None:
                speed_text = f"Speed: {speed:.1f}"
                cv2.putText(frame, speed_text, (x1, y2 + 20),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
        
        return frame
    
    def _get_object_color(self, obj_id: int) -> Tuple[int, int, int]:
        """ç²å–ç‰©ä»¶çš„å›ºå®šé¡è‰²"""
        if obj_id not in self.colors:
            # ä½¿ç”¨HSVè‰²å½©ç©ºé–“ç”Ÿæˆä¸åŒçš„é¡è‰²
            hue = (obj_id * 137.508) % 360  # é»ƒé‡‘è§’åº¦åˆ†ä½ˆ
            saturation = 0.7 + (obj_id % 3) * 0.1  # 0.7-1.0
            value = 0.8 + (obj_id % 2) * 0.2  # 0.8-1.0
            
            # è½‰æ›ç‚ºBGR
            rgb = colorsys.hsv_to_rgb(hue/360, saturation, value)
            bgr = tuple(int(c * 255) for c in rgb[::-1])  # RGB to BGR
            self.colors[obj_id] = bgr
        
        return self.colors[obj_id]
    
    def _update_trail(self, track_id: int, position: Tuple[int, int]):
        """æ›´æ–°ç§»å‹•è»Œè·¡"""
        if track_id not in self.trail_history:
            self.trail_history[track_id] = []
        
        self.trail_history[track_id].append(position)
        
        # é™åˆ¶è»Œè·¡é•·åº¦
        if len(self.trail_history[track_id]) > self.max_trail_length:
            self.trail_history[track_id].pop(0)
    
    def _draw_trail(self, frame: np.ndarray, track_id: int, color: Tuple[int, int, int]):
        """ç¹ªè£½ç§»å‹•è»Œè·¡"""
        if track_id not in self.trail_history:
            return
        
        points = self.trail_history[track_id]
        if len(points) < 2:
            return
        
        # ç¹ªè£½è»Œè·¡ç·š
        for i in range(1, len(points)):
            # æ ¹æ“šæ™‚é–“é è¿‘èª¿æ•´é€æ˜åº¦
            alpha = i / len(points)
            thickness = max(1, int(alpha * 3))
            
            # èª¿æ•´é¡è‰²äº®åº¦
            trail_color = tuple(int(c * alpha) for c in color)
            
            cv2.line(frame, points[i-1], points[i], trail_color, thickness)
        
        # ç¹ªè£½è»Œè·¡é»
        for i, point in enumerate(points):
            alpha = (i + 1) / len(points)
            radius = max(1, int(alpha * 3))
            trail_color = tuple(int(c * alpha) for c in color)
            cv2.circle(frame, point, radius, trail_color, -1)
    
    def _calculate_direction(self, track_id: int) -> Optional[float]:
        """è¨ˆç®—ç§»å‹•æ–¹å‘ï¼ˆè§’åº¦ï¼‰"""
        if track_id not in self.trail_history:
            return None
        
        points = self.trail_history[track_id]
        if len(points) < 2:
            return None
        
        # ä½¿ç”¨æœ€è¿‘çš„å¹¾å€‹é»è¨ˆç®—æ–¹å‘
        recent_points = points[-min(5, len(points)):]
        if len(recent_points) < 2:
            return None
        
        # è¨ˆç®—å¹³å‡ç§»å‹•æ–¹å‘
        dx_total, dy_total = 0, 0
        for i in range(1, len(recent_points)):
            dx = recent_points[i][0] - recent_points[i-1][0]
            dy = recent_points[i][1] - recent_points[i-1][1]
            dx_total += dx
            dy_total += dy
        
        if dx_total == 0 and dy_total == 0:
            return None
        
        # è¨ˆç®—è§’åº¦ï¼ˆå¼§åº¦è½‰åº¦ï¼‰
        angle = math.atan2(dy_total, dx_total) * 180 / math.pi
        return angle
    
    def _draw_direction_arrow(self, frame: np.ndarray, center: Tuple[int, int], 
                             angle: float, color: Tuple[int, int, int]):
        """ç¹ªè£½æ–¹å‘ç®­é ­"""
        cx, cy = center
        
        # ç®­é ­é•·åº¦
        arrow_length = 30
        
        # è¨ˆç®—ç®­é ­çµ‚é»
        end_x = int(cx + arrow_length * math.cos(math.radians(angle)))
        end_y = int(cy + arrow_length * math.sin(math.radians(angle)))
        
        # ç¹ªè£½ä¸»ç·š
        cv2.arrowedLine(frame, (cx, cy), (end_x, end_y), color, 3, tipLength=0.3)
        
        # æ·»åŠ æ–¹å‘æ–‡å­—
        direction_text = self._angle_to_direction_text(angle)
        text_x = end_x + 5
        text_y = end_y - 5
        
        cv2.putText(frame, direction_text, (text_x, text_y),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
    
    def _angle_to_direction_text(self, angle: float) -> str:
        """å°‡è§’åº¦è½‰æ›ç‚ºæ–¹å‘æ–‡å­—"""
        # æ­£è¦åŒ–è§’åº¦åˆ°0-360åº¦
        angle = angle % 360
        if angle < 0:
            angle += 360
        
        if 337.5 <= angle or angle < 22.5:
            return "â†’"  # å³
        elif 22.5 <= angle < 67.5:
            return "â†˜"  # å³ä¸‹
        elif 67.5 <= angle < 112.5:
            return "â†“"  # ä¸‹
        elif 112.5 <= angle < 157.5:
            return "â†™"  # å·¦ä¸‹
        elif 157.5 <= angle < 202.5:
            return "â†"  # å·¦
        elif 202.5 <= angle < 247.5:
            return "â†–"  # å·¦ä¸Š
        elif 247.5 <= angle < 292.5:
            return "â†‘"  # ä¸Š
        elif 292.5 <= angle < 337.5:
            return "â†—"  # å³ä¸Š
        else:
            return "?"
    
    def _calculate_speed(self, track_id: int) -> Optional[float]:
        """è¨ˆç®—ç§»å‹•é€Ÿåº¦ï¼ˆåƒç´ /å¹€ï¼‰"""
        if track_id not in self.trail_history:
            return None
        
        points = self.trail_history[track_id]
        if len(points) < 2:
            return None
        
        # è¨ˆç®—æœ€è¿‘å¹¾å€‹é»çš„å¹³å‡é€Ÿåº¦
        recent_points = points[-min(10, len(points)):]
        if len(recent_points) < 2:
            return None
        
        total_distance = 0
        for i in range(1, len(recent_points)):
            dx = recent_points[i][0] - recent_points[i-1][0]
            dy = recent_points[i][1] - recent_points[i-1][1]
            distance = math.sqrt(dx*dx + dy*dy)
            total_distance += distance
        
        # å¹³å‡é€Ÿåº¦ï¼ˆåƒç´ /å¹€ï¼‰
        avg_speed = total_distance / (len(recent_points) - 1)
        return avg_speed
    
    def _add_frame_info(self, frame: np.ndarray, frame_number: int, object_count: int) -> np.ndarray:
        """æ·»åŠ å¹€è³‡è¨Š"""
        height, width = frame.shape[:2]
        
        # æ·»åŠ åŠé€æ˜èƒŒæ™¯
        overlay = frame.copy()
        cv2.rectangle(overlay, (10, 10), (300, 80), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        
        # æ·»åŠ æ–‡å­—è³‡è¨Š
        info_text = [
            f"Frame: {frame_number}",
            f"Objects: {object_count}",
            f"Resolution: {width}x{height}"
        ]
        
        for i, text in enumerate(info_text):
            cv2.putText(frame, text, (15, 30 + i * 20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        return frame


# å…¨åŸŸæœå‹™å¯¦ä¾‹
# å»¶é²åˆå§‹åŒ–çš„å…¨åŸŸæœå‹™å¯¦ä¾‹  
_video_annotation_service = None

def get_video_annotation_service():
    """ç²å–å½±ç‰‡æ¨™è¨»æœå‹™å¯¦ä¾‹ï¼ˆå»¶é²åˆå§‹åŒ–ï¼‰"""
    global _video_annotation_service
    if _video_annotation_service is None:
        _video_annotation_service = VideoAnnotationService()
    return _video_annotation_service

# å‘å¾Œç›¸å®¹æ€§åˆ¥å
def video_annotation_service():
    return get_video_annotation_service()
